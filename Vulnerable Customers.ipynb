{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vulnerable Customers\n",
    "\n",
    "4 key drivers of vulnerability:\n",
    "1. Health – disabilities or illnesses that affect the ability to carry out day-to-day tasks\n",
    "2. Life Events – major life events such as bereavement, job loss or relationship breakdown\n",
    "3. Resilience – low ability to withstand financial or emotional shocks\n",
    "4. Capability – low knowledge of financial matters or low confidence in managing money (financial capability) and low capability in other relevant areas such as literacy, or digital skills\n",
    "\n",
    "Normalising this into 16 distinct topics:\n",
    "1. Learning disability\n",
    "2. Low income\n",
    "3. Mental health issues\n",
    "4. Health problems\n",
    "5. Being a carer\n",
    "6. Age\n",
    "7. Physical disability\n",
    "8. Lack of connectivity\n",
    "9. Living alone\n",
    "10. Lone parent\n",
    "11. Loss of income\n",
    "12. Leaving care\n",
    "13. Bereavement\n",
    "14. Relationship breakdown\n",
    "15. Release from prison\n",
    "16. Legal proceedings\n",
    "\n",
    "Potential approaches:\n",
    "1. Bag-of-Words: will need enough training data for us to come to some sensible features. But this will essentially be goal-seeking because only sensible features will be synonyms of the topic at hand.\n",
    "2. Similarity measure: use wordnet based similarity measure to monitor stream of text for mention of words close in meaning to these topics, ie. synonyms.\n",
    "\n",
    "Winner = I will use the similarity measure as this will also not require large training data to be collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T18:46:31.059137Z",
     "start_time": "2020-09-26T18:46:31.039321Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:10:58.502331Z",
     "start_time": "2020-09-29T18:10:58.494727Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_cleaner(phrase):\n",
    "    '''Removes stopwords, punctuation from text, and converts into a list of word tokens\n",
    "    Args:\n",
    "    phrase = text string\n",
    "    \n",
    "    Outputs:\n",
    "    list of word tokens\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update([',','.'])\n",
    "    word_tokens = word_tokenize(phrase)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "def synonym_scorer(filtered_sentence, topic, sim_thresh = 0.6):\n",
    "    '''\n",
    "    For each word in a sentence, retrieves the synonym set. For each synonym we measure the wup_similarity\n",
    "    to the topic at hand. If similarity > sim_threshold, the topic is said to have been mentioned.\n",
    "    \n",
    "    Args:\n",
    "    filtered_sentence = tokenized sentence, preferrably stripped of stopwords\n",
    "    topic = Synset of the topic in question.\n",
    "    sim_threshold = threshold for topic similarity (default = 0.6)\n",
    "    \n",
    "    Outputs:\n",
    "    Integer count of the number of mentions of the topic in the filtered_sentence\n",
    "    '''\n",
    "    word_scores = []\n",
    "    for w in range(len(filtered_sentence)):\n",
    "        syns = wordnet.synsets(filtered_sentence[w])\n",
    "        syns_sim = [topic.wup_similarity(syns[s]) for s in range(len(syns))]\n",
    "        syns_sim = [x if x is not None else 0 for x in syns_sim]\n",
    "        syns_sim = np.max([1 if x > sim_thresh else 0 for x in syns_sim])\n",
    "        word_scores.append(syns_sim)\n",
    "    return np.sum(word_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:10:59.367520Z",
     "start_time": "2020-09-29T18:10:59.362230Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_dictionary = {'disability': 'disabled.n.01',\n",
    "                    'death': 'die.v.01',\n",
    "                    'health problems': 'ill.a.01',\n",
    "                    'being a carer': 'care.v.02',\n",
    "                    'living alone': 'alone.s.01'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:10:59.818014Z",
     "start_time": "2020-09-29T18:10:59.681056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disability': 3,\n",
       " 'death': 1,\n",
       " 'health problems': 1,\n",
       " 'being a carer': 0,\n",
       " 'living alone': 0}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'People grieve in different ways and there is no right or wrong way to react to the death of a colleague, friend or family member.  Many people find it helpful to reach out and talk to someone about their feelings, other may wish to deal with the loss in private. Disability ill disabled'\n",
    "\n",
    "sim_scores = {}\n",
    "for topic in list(topic_dictionary.keys()):\n",
    "    topic_synset = wordnet.synset(topic_dictionary['{}'.format(topic)])\n",
    "    sim_scores['{}'.format(topic)] = synonym_scorer(sentence_cleaner(phrase), topic_synset)\n",
    "sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vulnerablecustomers",
   "language": "python",
   "name": "vulnerablecustomers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
